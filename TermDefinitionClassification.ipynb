{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ffdc376",
   "metadata": {},
   "source": [
    "# Notebook Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e9f259",
   "metadata": {},
   "source": [
    "In this notebook, I explore a a fine tuning task for a token classification problem using the BERT-base-cased model.\n",
    "\n",
    "Use Case:\n",
    "- The goal is to classify words in a text line as either part of a term or a definition.\n",
    "- All training samples were collected from a Star Wars encyclopedia and are used solely for educational purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b0b42c",
   "metadata": {},
   "source": [
    "# Module Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e66ba39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Utility classes from the Hugging Face transformers library\n",
    "from transformers import DataCollatorForTokenClassification #Prepares input data for the model\n",
    "from transformers import AutoModelForTokenClassification #Loads a pretrained model suited for token classification\n",
    "from transformers import TrainingArguments #Class to define all the hyperparameters and configuration for training the model\n",
    "from transformers import Trainer #API for training Pytorch models\n",
    "from transformers import AutoTokenizer #\n",
    "\n",
    "from datasets import Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f8e725",
   "metadata": {},
   "source": [
    "# Data Load and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b0ebaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "term_def_df = pd.read_csv('./sample_train.csv',index_col='idx')\n",
    "test_df = pd.read_csv('./test.csv',index_col='idx')\n",
    "\n",
    "term_def_df.drop(columns=['block','page'],inplace=True)\n",
    "term_def_df['text'] = term_def_df['term'].values + ' ' + term_def_df['definition'].values\n",
    "\n",
    "test_df.drop(columns=['block','term','page'],inplace=True)\n",
    "test_df.rename(columns={'definition':'text'},inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa2a480",
   "metadata": {},
   "source": [
    "# Tokenizer Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c85095",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Bert model works with a maximum of 512 characters, for text longer than that there are 3 options:\n",
    "#1) In a posterior function it can be included the option to truncate\n",
    "#2) Drop the samples with more then 512 characters\n",
    "#3) Split into chunks\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac49d48",
   "metadata": {},
   "source": [
    "# Label Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1f0eca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "id2label = {\n",
    "    0: \"O\",        # Outside of a term/definition\n",
    "    1: \"B-TERM\",   # Beginning of a term\n",
    "    2: \"I-TERM\",   # Inside a term\n",
    "    3: \"B-DEF\",    # Beginning of a definition\n",
    "    4: \"I-DEF\",    # Inside a definition\n",
    "}\n",
    "label2id = {label: id for id, label in id2label.items()}\n",
    "num_labels = len(id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9638a50b",
   "metadata": {},
   "source": [
    "# Tokenize and Aling Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6712d598",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_and_align_labels(row):\n",
    "\n",
    "    full_text = row['text']\n",
    "    term_text = row['term']\n",
    "    def_text = row['definition']\n",
    "\n",
    "    tokenized_inputs = tokenizer(\n",
    "        full_text, \n",
    "        truncation=True, \n",
    "        max_length=512, #Truncate the text to 512 characters\n",
    "        return_offsets_mapping=True\n",
    "    )\n",
    "    \n",
    "    labels = [-100] * len(tokenized_inputs['input_ids'])  # -100 is used to ignore tokens in loss calculation\n",
    "    \n",
    "    # Find the start and end character positions of the term and definition in the full text\n",
    "    term_start_char = full_text.find(term_text)\n",
    "    term_end_char = term_start_char + len(term_text)\n",
    "    def_start_char = full_text.find(def_text, term_end_char)\n",
    "    def_end_char = def_start_char + len(def_text)\n",
    "    \n",
    "    for i,offset in enumerate(tokenized_inputs['offset_mapping']):\n",
    "        #Retrieve the start and end character positions of the token respective to the full text\n",
    "        token_start_char, token_end_char = offset\n",
    "        \n",
    "        if token_start_char == token_end_char and token_start_char == 0 and i > 0:\n",
    "            # If the token is empty (e.g., a space or punctuation), skip it\n",
    "            continue\n",
    "        \n",
    "        #Compare the token's character positions with the term and definition positions to assign corresponding labels\n",
    "        if  term_start_char <= token_start_char < term_end_char:\n",
    "            if token_start_char == term_start_char:\n",
    "                labels[i] = label2id['B-TERM']\n",
    "            else:\n",
    "                labels[i] = label2id['I-TERM']\n",
    "        elif def_start_char <= token_start_char < def_end_char:\n",
    "            if token_start_char == def_start_char:\n",
    "                labels[i] = label2id['B-DEF']\n",
    "            else:\n",
    "                labels[i] = label2id['I-DEF']\n",
    "        else:\n",
    "            labels[i] = label2id['O']\n",
    "    \n",
    "    tokenized_inputs['labels'] = labels\n",
    "    tokenized_inputs.pop('offset_mapping')  # Remove offset_mapping as it's not needed for training\n",
    "    tokenized_inputs.pop('token_type_ids')  # Remove offset_mapping as it's not needed for training\n",
    "    \n",
    "    return tokenized_inputs\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b72179",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = term_def_df.apply(tokenize_and_align_labels,axis=1).tolist()\n",
    "dataset_dict = {key: [d[key] for d in processed] for key in processed[0].keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015ed6a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 800\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "term_def_dataset = Dataset.from_dict(dataset_dict)\n",
    "term_def_dataset = term_def_dataset.train_test_split(test_size=0.2,seed=42)\n",
    "term_def_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478788ac",
   "metadata": {},
   "source": [
    "# Data Collation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06a37cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bface3f",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6068405",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    'bert-base-cased', \n",
    "    num_labels= num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8feae3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "class PrintCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        print(f\"[Step {state.global_step}] {logs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59c00c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir='./term_def_model',\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_strategy='steps',\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    #push_to_hub=False,\n",
    "    disable_tqdm=False,\n",
    "    fp16=True,\n",
    "    no_cuda=False # Set to True if you don't have a GPU\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = args,\n",
    "    train_dataset = term_def_dataset['train'],\n",
    "    eval_dataset = term_def_dataset['test'],\n",
    "    #processing_class = tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[PrintCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a699fcf1",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "403ea7b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/300 01:26, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.014588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>0.009608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.011521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 10] {'loss': 0.9207, 'grad_norm': 1.6174964904785156, 'learning_rate': 4.9e-05, 'epoch': 0.1}\n",
      "[Step 20] {'loss': 0.1327, 'grad_norm': 0.4402577579021454, 'learning_rate': 4.7333333333333336e-05, 'epoch': 0.2}\n",
      "[Step 30] {'loss': 0.0793, 'grad_norm': 0.40364736318588257, 'learning_rate': 4.566666666666667e-05, 'epoch': 0.3}\n",
      "[Step 40] {'loss': 0.0233, 'grad_norm': 0.9799392223358154, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.4}\n",
      "[Step 50] {'loss': 0.0077, 'grad_norm': 0.03879417106509209, 'learning_rate': 4.233333333333334e-05, 'epoch': 0.5}\n",
      "[Step 60] {'loss': 0.0063, 'grad_norm': 0.587995171546936, 'learning_rate': 4.066666666666667e-05, 'epoch': 0.6}\n",
      "[Step 70] {'loss': 0.0084, 'grad_norm': 0.5140938758850098, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.7}\n",
      "[Step 80] {'loss': 0.0064, 'grad_norm': 0.8027336597442627, 'learning_rate': 3.733333333333334e-05, 'epoch': 0.8}\n",
      "[Step 90] {'loss': 0.0132, 'grad_norm': 0.6628711223602295, 'learning_rate': 3.566666666666667e-05, 'epoch': 0.9}\n",
      "[Step 100] {'loss': 0.0125, 'grad_norm': 0.4584032893180847, 'learning_rate': 3.4000000000000007e-05, 'epoch': 1.0}\n",
      "[Step 100] {'eval_loss': 0.014588439837098122, 'eval_runtime': 0.7665, 'eval_samples_per_second': 260.924, 'eval_steps_per_second': 32.616, 'epoch': 1.0}\n",
      "[Step 110] {'loss': 0.0111, 'grad_norm': 0.1412716954946518, 'learning_rate': 3.233333333333333e-05, 'epoch': 1.1}\n",
      "[Step 120] {'loss': 0.003, 'grad_norm': 0.3972190022468567, 'learning_rate': 3.066666666666667e-05, 'epoch': 1.2}\n",
      "[Step 130] {'loss': 0.0031, 'grad_norm': 0.011569906026124954, 'learning_rate': 2.9e-05, 'epoch': 1.3}\n",
      "[Step 140] {'loss': 0.0062, 'grad_norm': 0.030552135780453682, 'learning_rate': 2.733333333333333e-05, 'epoch': 1.4}\n",
      "[Step 150] {'loss': 0.0013, 'grad_norm': 0.21473710238933563, 'learning_rate': 2.5666666666666666e-05, 'epoch': 1.5}\n",
      "[Step 160] {'loss': 0.0018, 'grad_norm': 0.08231645077466965, 'learning_rate': 2.4e-05, 'epoch': 1.6}\n",
      "[Step 170] {'loss': 0.0064, 'grad_norm': 0.09528837352991104, 'learning_rate': 2.2333333333333335e-05, 'epoch': 1.7}\n",
      "[Step 180] {'loss': 0.0086, 'grad_norm': 0.18577715754508972, 'learning_rate': 2.0666666666666666e-05, 'epoch': 1.8}\n",
      "[Step 190] {'loss': 0.0063, 'grad_norm': 0.605229377746582, 'learning_rate': 1.9e-05, 'epoch': 1.9}\n",
      "[Step 200] {'loss': 0.0093, 'grad_norm': 0.5657298564910889, 'learning_rate': 1.7333333333333336e-05, 'epoch': 2.0}\n",
      "[Step 200] {'eval_loss': 0.009608153253793716, 'eval_runtime': 0.7676, 'eval_samples_per_second': 260.569, 'eval_steps_per_second': 32.571, 'epoch': 2.0}\n",
      "[Step 210] {'loss': 0.0008, 'grad_norm': 0.018061643466353416, 'learning_rate': 1.5666666666666667e-05, 'epoch': 2.1}\n",
      "[Step 220] {'loss': 0.0023, 'grad_norm': 0.12006457895040512, 'learning_rate': 1.4000000000000001e-05, 'epoch': 2.2}\n",
      "[Step 230] {'loss': 0.001, 'grad_norm': 0.08012765645980835, 'learning_rate': 1.2333333333333334e-05, 'epoch': 2.3}\n",
      "[Step 240] {'loss': 0.001, 'grad_norm': 0.27481287717819214, 'learning_rate': 1.0666666666666667e-05, 'epoch': 2.4}\n",
      "[Step 250] {'loss': 0.0013, 'grad_norm': 0.004644796717911959, 'learning_rate': 9e-06, 'epoch': 2.5}\n",
      "[Step 260] {'loss': 0.0012, 'grad_norm': 0.07649922370910645, 'learning_rate': 7.333333333333334e-06, 'epoch': 2.6}\n",
      "[Step 270] {'loss': 0.0003, 'grad_norm': 0.007242321036756039, 'learning_rate': 5.666666666666667e-06, 'epoch': 2.7}\n",
      "[Step 280] {'loss': 0.0023, 'grad_norm': 0.006198633462190628, 'learning_rate': 4.000000000000001e-06, 'epoch': 2.8}\n",
      "[Step 290] {'loss': 0.0012, 'grad_norm': 0.11090991646051407, 'learning_rate': 2.3333333333333336e-06, 'epoch': 2.9}\n",
      "[Step 300] {'loss': 0.0007, 'grad_norm': 0.11014232039451599, 'learning_rate': 6.666666666666667e-07, 'epoch': 3.0}\n",
      "[Step 300] {'eval_loss': 0.011521442793309689, 'eval_runtime': 0.7472, 'eval_samples_per_second': 267.65, 'eval_steps_per_second': 33.456, 'epoch': 3.0}\n",
      "[Step 300] {'train_runtime': 87.5969, 'train_samples_per_second': 27.398, 'train_steps_per_second': 3.425, 'total_flos': 293913746976720.0, 'train_loss': 0.04265157299737136, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=300, training_loss=0.04265157299737136, metrics={'train_runtime': 87.5969, 'train_samples_per_second': 27.398, 'train_steps_per_second': 3.425, 'total_flos': 293913746976720.0, 'train_loss': 0.04265157299737136, 'epoch': 3.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f4b624",
   "metadata": {},
   "source": [
    "# Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897165f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best checkpoint: ./term_def_model/checkpoint-200\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "best_ckpt = trainer.state.best_model_checkpoint\n",
    "print('Best checkpoint: {}'.format(best_ckpt))\n",
    "\n",
    "if best_ckpt:\n",
    "    model = AutoModelForTokenClassification.from_pretrained(best_ckpt)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(best_ckpt)\n",
    "\n",
    "pipe = pipeline(\n",
    "    'token-classification',\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    aggregation_strategy=\"simple\",\n",
    "    device=0 #0 for GPU and -1 for CPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed60874d",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_texts = list(test_df['text'])\n",
    "predictions = pipe(raw_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a62d6977",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "\n",
    "for i, (text,pred) in enumerate(zip(raw_texts,predictions)):\n",
    "    terms_entities = [p for p in pred if p['entity_group'] == 'TERM']\n",
    "    defs_entities = [p for p in pred if p['entity_group'] == 'DEF']\n",
    "    \n",
    "    extrated_term = \"\"\n",
    "    extrated_def = \"\"\n",
    "    \n",
    "    if terms_entities:\n",
    "        extrated_term = terms_entities[0]['word']\n",
    "    \n",
    "    if defs_entities:\n",
    "        extrated_def = defs_entities[0]['word']\n",
    "        \n",
    "    records.append({\n",
    "        'original_text': text,\n",
    "        'predicted_term': extrated_term,\n",
    "        'predicted_definition': extrated_def\n",
    "    })\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3087b99c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>predicted_term</th>\n",
       "      <th>predicted_definition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AS assassin droid A repurposed assassin droid ...</td>\n",
       "      <td>AS assassin droid</td>\n",
       "      <td>A repurposed assassin droid programmed to act ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A99 Aquata Breather A compact breathing appara...</td>\n",
       "      <td>A99 Aquata B</td>\n",
       "      <td>##reath</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AA-23, Detention Block The block that held Pri...</td>\n",
       "      <td>AA - 23, Detention Block</td>\n",
       "      <td>The block that held Princess Leia Organa aboar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aarrba the Hutt A kindhearted Hutt who owned a...</td>\n",
       "      <td>Aarrba the Hutt</td>\n",
       "      <td>A kindhearted Hutt who owned a starship dock a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAT (armored assault tank) The front line of T...</td>\n",
       "      <td>AAT ( armored assault tank )</td>\n",
       "      <td>The front line of Trade Federation armored inf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Aavman Extravagance The manufacturer of such l...</td>\n",
       "      <td>Aavman Extravagance</td>\n",
       "      <td>The manufacturer of such luxury vessels as the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Abran system Site of the Abran Belt and the B’...</td>\n",
       "      <td>Abran system</td>\n",
       "      <td>Site of the Abran Belt and the B ’ Knos mining...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Abrion sector A bread basket of sorts for the ...</td>\n",
       "      <td>Abrion sector</td>\n",
       "      <td>A bread basket of sorts for the galaxy, the Ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Abyssin grafting patch A medical supply from K...</td>\n",
       "      <td>Abyssin grafting patch</td>\n",
       "      <td>A medical supply from Kirgalis Pharmaceutical ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>acceleration chair, acceleration couch A gener...</td>\n",
       "      <td>acceleration chair, acceleration couch</td>\n",
       "      <td>A generic term for g - force - absorbing seats...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>acceleration compensator A device that generat...</td>\n",
       "      <td>acceleration compensator</td>\n",
       "      <td>A device that generated a type of artificial g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>acceleration facility A term describing variou...</td>\n",
       "      <td>acceleration facility</td>\n",
       "      <td>A term describing various facilities construct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>acceleration straps These passenger-safety har...</td>\n",
       "      <td>acceleration straps</td>\n",
       "      <td>These passenger - safety harnesses were usuall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Accipiptero Acclamator-ciass assault ship Mass...</td>\n",
       "      <td>Accipiptero</td>\n",
       "      <td>Acclamator - ciass assault ship Massive transp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Acherin Jedi Master Garen Muln led Republic fo...</td>\n",
       "      <td>Acherin</td>\n",
       "      <td>Jed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Acklays  acklay A) The first few years of the ...</td>\n",
       "      <td>Acklays acklay A )</td>\n",
       "      <td>The first few years of the New Republic were e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>acrobat droid A generic term describing any au...</td>\n",
       "      <td>acrobat droid</td>\n",
       "      <td>A generic term describing any automaton progra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Action Tidings An Ugnaught news broadcast made...</td>\n",
       "      <td>Action Tidings</td>\n",
       "      <td>An Ugnaught news broadcast made available to d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Adega system A binary star system located in t...</td>\n",
       "      <td>Adega system</td>\n",
       "      <td>A binary star system located in the Outer Rim ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Adegan crystals Precious crystals used by the ...</td>\n",
       "      <td>Adegan crystals</td>\n",
       "      <td>Precious crystals used by the © Jedi to constr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Adegan eel A disgusting Cane Adiss snake-like ...</td>\n",
       "      <td>Adegan eel</td>\n",
       "      <td>A disgusting Cane Adiss snake - like venomous ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Admiral Korvin Raith Sienar’s flagship on his ...</td>\n",
       "      <td>Admiral Korvin</td>\n",
       "      <td>Raith Sienar ’ s flagship on his mission to Zo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Aduba system A remote system in the Outer Rim ...</td>\n",
       "      <td>Aduba system</td>\n",
       "      <td>A remote system in the Outer Rim Territories f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Adz patrol destroyer (Adz-class destroyer) An ...</td>\n",
       "      <td>Adz patrol destroyer ( Adz - class destroyer )</td>\n",
       "      <td>An Imperial deep - space patrol ship. The Adz ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Aggaba the Hutt An assistant to Jemba the Hutt...</td>\n",
       "      <td>Aggaba the Hutt</td>\n",
       "      <td>An assistant to Jemba the Hutt of the Offworld...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Ahto City Capital city of Manaan, built above ...</td>\n",
       "      <td>Ahto City</td>\n",
       "      <td>Capital city of Manaan, built above the surfac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Ailon Nova Guard A military unit in the Ailon ...</td>\n",
       "      <td>Ailon Nova Guard</td>\n",
       "      <td>A military unit in the Ailon system known for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Air-2 racing swoop A TaggeCo heavy swoop bike ...</td>\n",
       "      <td>Air - 2 racing swoop</td>\n",
       "      <td>A TaggeCo heavy swoop bike with maneuvering fl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Aiwha Squad A Republic commando pod active dur...</td>\n",
       "      <td>Aiwha Squad</td>\n",
       "      <td>A Republic commando pod active during the Clon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Ajuur the Hutt The owner of a combat arena on ...</td>\n",
       "      <td>Ajuur the Hutt</td>\n",
       "      <td>The owner of a combat arena on Taris about 4, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        original_text  \\\n",
       "0   AS assassin droid A repurposed assassin droid ...   \n",
       "1   A99 Aquata Breather A compact breathing appara...   \n",
       "2   AA-23, Detention Block The block that held Pri...   \n",
       "3   Aarrba the Hutt A kindhearted Hutt who owned a...   \n",
       "4   AAT (armored assault tank) The front line of T...   \n",
       "5   Aavman Extravagance The manufacturer of such l...   \n",
       "6   Abran system Site of the Abran Belt and the B’...   \n",
       "7   Abrion sector A bread basket of sorts for the ...   \n",
       "8   Abyssin grafting patch A medical supply from K...   \n",
       "9   acceleration chair, acceleration couch A gener...   \n",
       "10  acceleration compensator A device that generat...   \n",
       "11  acceleration facility A term describing variou...   \n",
       "12  acceleration straps These passenger-safety har...   \n",
       "13  Accipiptero Acclamator-ciass assault ship Mass...   \n",
       "14  Acherin Jedi Master Garen Muln led Republic fo...   \n",
       "15  Acklays  acklay A) The first few years of the ...   \n",
       "16  acrobat droid A generic term describing any au...   \n",
       "17  Action Tidings An Ugnaught news broadcast made...   \n",
       "18  Adega system A binary star system located in t...   \n",
       "19  Adegan crystals Precious crystals used by the ...   \n",
       "20  Adegan eel A disgusting Cane Adiss snake-like ...   \n",
       "21  Admiral Korvin Raith Sienar’s flagship on his ...   \n",
       "22  Aduba system A remote system in the Outer Rim ...   \n",
       "23  Adz patrol destroyer (Adz-class destroyer) An ...   \n",
       "24  Aggaba the Hutt An assistant to Jemba the Hutt...   \n",
       "25  Ahto City Capital city of Manaan, built above ...   \n",
       "26  Ailon Nova Guard A military unit in the Ailon ...   \n",
       "27  Air-2 racing swoop A TaggeCo heavy swoop bike ...   \n",
       "28  Aiwha Squad A Republic commando pod active dur...   \n",
       "29  Ajuur the Hutt The owner of a combat arena on ...   \n",
       "\n",
       "                                    predicted_term  \\\n",
       "0                                AS assassin droid   \n",
       "1                                     A99 Aquata B   \n",
       "2                         AA - 23, Detention Block   \n",
       "3                                  Aarrba the Hutt   \n",
       "4                     AAT ( armored assault tank )   \n",
       "5                              Aavman Extravagance   \n",
       "6                                     Abran system   \n",
       "7                                    Abrion sector   \n",
       "8                           Abyssin grafting patch   \n",
       "9           acceleration chair, acceleration couch   \n",
       "10                        acceleration compensator   \n",
       "11                           acceleration facility   \n",
       "12                             acceleration straps   \n",
       "13                                     Accipiptero   \n",
       "14                                         Acherin   \n",
       "15                              Acklays acklay A )   \n",
       "16                                   acrobat droid   \n",
       "17                                  Action Tidings   \n",
       "18                                    Adega system   \n",
       "19                                 Adegan crystals   \n",
       "20                                      Adegan eel   \n",
       "21                                  Admiral Korvin   \n",
       "22                                    Aduba system   \n",
       "23  Adz patrol destroyer ( Adz - class destroyer )   \n",
       "24                                 Aggaba the Hutt   \n",
       "25                                       Ahto City   \n",
       "26                                Ailon Nova Guard   \n",
       "27                            Air - 2 racing swoop   \n",
       "28                                     Aiwha Squad   \n",
       "29                                  Ajuur the Hutt   \n",
       "\n",
       "                                 predicted_definition  \n",
       "0   A repurposed assassin droid programmed to act ...  \n",
       "1                                             ##reath  \n",
       "2   The block that held Princess Leia Organa aboar...  \n",
       "3   A kindhearted Hutt who owned a starship dock a...  \n",
       "4   The front line of Trade Federation armored inf...  \n",
       "5   The manufacturer of such luxury vessels as the...  \n",
       "6   Site of the Abran Belt and the B ’ Knos mining...  \n",
       "7   A bread basket of sorts for the galaxy, the Ab...  \n",
       "8   A medical supply from Kirgalis Pharmaceutical ...  \n",
       "9   A generic term for g - force - absorbing seats...  \n",
       "10  A device that generated a type of artificial g...  \n",
       "11  A term describing various facilities construct...  \n",
       "12  These passenger - safety harnesses were usuall...  \n",
       "13  Acclamator - ciass assault ship Massive transp...  \n",
       "14                                                Jed  \n",
       "15  The first few years of the New Republic were e...  \n",
       "16  A generic term describing any automaton progra...  \n",
       "17  An Ugnaught news broadcast made available to d...  \n",
       "18  A binary star system located in the Outer Rim ...  \n",
       "19  Precious crystals used by the © Jedi to constr...  \n",
       "20  A disgusting Cane Adiss snake - like venomous ...  \n",
       "21  Raith Sienar ’ s flagship on his mission to Zo...  \n",
       "22  A remote system in the Outer Rim Territories f...  \n",
       "23  An Imperial deep - space patrol ship. The Adz ...  \n",
       "24  An assistant to Jemba the Hutt of the Offworld...  \n",
       "25  Capital city of Manaan, built above the surfac...  \n",
       "26  A military unit in the Ailon system known for ...  \n",
       "27  A TaggeCo heavy swoop bike with maneuvering fl...  \n",
       "28  A Republic commando pod active during the Clon...  \n",
       "29  The owner of a combat arena on Taris about 4, ...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df = pd.DataFrame(records)\n",
    "output_df.head(30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
